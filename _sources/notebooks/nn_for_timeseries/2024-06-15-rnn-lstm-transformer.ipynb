{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Networks\n",
    "Author: [Zeel B Patel](https://patel-zeel.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from beartype import beartype\n",
    "from jaxtyping import Float, jaxtyped\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Union, Tuple\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Next Character Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the specifics, it'd be helpful to define a tiny problem (without loss of generality).\n",
    "\n",
    "* Problm: Given a sequence of characters, predict the next character in the sequence.\n",
    "* Example input: \"hell\"\n",
    "* Example output: \"o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource: [Andrej Karpathy | CS231n Winter 2016: Lecture 10: Recurrent Neural Networks, Image Captioning, LSTM](https://youtu.be/yCC09vCHzF8?feature=shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image source: [https://cs231n.github.io/assets/rnn/types.png](https://cs231n.github.io/assets/rnn/types.png)\n",
    "![image.png](https://cs231n.github.io/assets/rnn/types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reccurrent Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_t = f(h_{t-1}, x_t)\n",
    "$$\n",
    "\n",
    "* $h_t$ is the hidden state at time $t$.\n",
    "* $h_{t-1}$ is the hidden state at time $t-1$.\n",
    "* $x_t$ is the input at time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Question\n",
    ":class: important\n",
    "But, what do we do with these hidden states? We want to predict the next character in the sequence.\n",
    "```\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: hint\n",
    "The reccurent equation just represents the \"encoder\". We need a \"decoder\" to predict the next character in the sequence.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Encoder ($f$): Encodes an input into a hidden state.\n",
    "* Decoder ($g$): Decodes the hidden state into an output.\n",
    "\n",
    "```{admonition} Thought\n",
    ":class: important\n",
    "This looks similar to vanilla neural network with one hidden layer. The hidden layer is encoding the input and the output layer is decoding the hidden state.\n",
    "```\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: hint\n",
    "Yes, we can generelize \"Encoder-Decoder\" terminology to vanilla neural networks as well.\n",
    "\n",
    "| | Encoder | Decoder |\n",
    "|---|---|---|\n",
    "| Vanilla NNs | $h = f(x)$ | $y = g(h)$ | \n",
    "| RNNs | $h_t = f(h_{t-1}, x_t)$ | $y_t = g(h_t)$ |\n",
    "| Another way? | $h_{t-1} = f(x_{t-1})$ | $y_t = g(h_{t-1}, x_t)$ |\n",
    "\n",
    "Another way is related to [Neural processes](https://yanndubs.github.io/Neural-Process-Family/text/Intro.html). Okay, back to the focus of this notebook.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, feat_dim: int, hidden_dim: int):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        self.w_x = nn.Parameter(torch.randn(feat_dim, hidden_dim) / np.sqrt(feat_dim * hidden_dim)) \n",
    "        self.w_h = nn.Parameter(torch.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim * hidden_dim))\n",
    "        self.b = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        \n",
    "    @jaxtyped(typechecker=beartype)\n",
    "    def forward(self, \n",
    "                x: Float[Tensor, \"batch_dim feat_dim\"], \n",
    "                h_prev: Union[Float[Tensor, \"batch_dim hidden_dim\"], None] = None\n",
    "                ) -> Float[Tensor, \"batch_dim hidden_dim\"]:\n",
    "        if h_prev is None:\n",
    "            h = x @ self.w_x + self.b\n",
    "        else:\n",
    "            h = x @ self.w_x + h_prev @ self.w_h + self.b\n",
    "        return F.relu(h)\n",
    "    \n",
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, output_dim: int):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.w = nn.Parameter(torch.randn(hidden_dim, output_dim) / np.sqrt(hidden_dim * output_dim))\n",
    "        self.b = nn.Parameter(torch.zeros(output_dim))\n",
    "        \n",
    "    @jaxtyped(typechecker=beartype)\n",
    "    def forward(self, h: Float[Tensor, \"batch_dim hidden_dim\"]) -> Float[Tensor, \"batch_dim output_dim\"]:\n",
    "        x = h @ self.w + self.b\n",
    "        return x\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, feat_dim: int, hidden_dim: int, output_dim: int, sequence_len: int):\n",
    "        super(RNN, self).__init__()\n",
    "        self.encoder = RNNEncoder(feat_dim, hidden_dim)\n",
    "        self.decoder = RNNDecoder(hidden_dim, output_dim)\n",
    "        self.sequence_len = sequence_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, x: Float[Tensor, \"batch_dim sequence_len feat_dim\"]) -> Float[Tensor, \"batch_dim feat_dim\"]:\n",
    "        self.h = None # initial hidden state\n",
    "        \n",
    "        # encode\n",
    "        for seq_idx in range(1, self.sequence_len):\n",
    "            self.h = self.encoder(x[:, seq_idx, :], self.h)\n",
    "        \n",
    "        # decode\n",
    "        y_hat = self.decoder(self.h)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking with dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_batch_dim = 2\n",
    "tmp_feat_dim = 3\n",
    "tmp_output_dim = 7\n",
    "tmp_hidden_dim = 5\n",
    "tmp_sequence_len = 10\n",
    "x = torch.randn(tmp_batch_dim, tmp_sequence_len, tmp_feat_dim)\n",
    "model = RNN(tmp_feat_dim, tmp_hidden_dim, tmp_output_dim, tmp_sequence_len)\n",
    "assert model(x).shape == (tmp_batch_dim, tmp_output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting some training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model on abstracts of multiple papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kaggle\n",
    "\n",
    "# kaggle.api.dataset_download_files(\"Cornell-University/arxiv\", path=\"data\", unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/arxiv-metadata-oai-snapshot.json\", \"r\") as f:\n",
    "    data = []\n",
    "    for i in range(100):\n",
    "        data.append(json.loads(f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set('\\n'.join([d[\"abstract\"] for d in data]))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(sparse_output=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(sparse_output=False)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(sparse_output=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = OneHotEncoder(sparse_output=False)\n",
    "tokenizer.fit(np.array(list(vocab)).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 93)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.transform(np.array([\"a\", \"b\", \"c\"]).reshape(3, 1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402b76293973457b89f8d9dba95c9b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78641, 20) (78641, 1)\n",
      "(78641, 20, 93) (78641, 93)\n"
     ]
    }
   ],
   "source": [
    "sequence_len = 20\n",
    "inputs = []\n",
    "outputs = []\n",
    "for d in tqdm(data):\n",
    "    abstract = d[\"abstract\"].replace(\"\\n\", \" \")\n",
    "    for i in range(len(abstract) - sequence_len):\n",
    "        input_seq = abstract[i:i+sequence_len]\n",
    "        output_token = abstract[i+sequence_len]\n",
    "        inputs.append(np.array(list(input_seq)))\n",
    "        outputs.append(np.array(list(output_token)))\n",
    "        \n",
    "inputs = np.stack(inputs)\n",
    "outputs = np.stack(outputs)\n",
    "print(inputs.shape, outputs.shape)\n",
    "inputs = tokenizer.transform(inputs.reshape(-1, 1)).reshape(inputs.shape[0], sequence_len, -1)\n",
    "outputs = tokenizer.transform(outputs.reshape(-1, 1)).reshape(outputs.shape[0], -1)\n",
    "print(inputs.shape, outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cd83b5c761464b9fe66c0ed1f6d19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 100\n",
    "hidden_dim = 1024\n",
    "\n",
    "train_inputs = inputs[:int(0.8*len(inputs))]\n",
    "train_outputs = outputs[:int(0.8*len(outputs))]\n",
    "test_inputs = inputs[int(0.8*len(inputs)):]\n",
    "test_outputs = outputs[int(0.8*len(outputs)):]\n",
    "\n",
    "dataset = TensorDataset(torch.tensor(train_inputs, dtype=torch.float32).to(device), torch.tensor(train_outputs, dtype=torch.float32).to(device))\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "feat_dim = len(vocab)\n",
    "output_dim = len(vocab)\n",
    "\n",
    "model = RNN(feat_dim, hidden_dim, output_dim, sequence_len)\n",
    "# model = torch.compile(model)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "iter_losses = []\n",
    "epoch_losses = []\n",
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    loss_accum = 0\n",
    "    for x, y in dataloader:\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat, y.argmax(dim=1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iter_losses.append(loss.item())\n",
    "        loss_accum += loss.item()\n",
    "    epoch_losses.append(loss_accum / len(dataloader))\n",
    "    pbar.set_postfix({\"loss\": epoch_losses[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafb6fa8ee9b440f8064ddca3a977f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(range(20))\n",
    "model.train()\n",
    "for epoch in pbar:\n",
    "    loss_accum = 0\n",
    "    for x, y in dataloader:\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat, y.argmax(dim=1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iter_losses.append(loss.item())\n",
    "        loss_accum += loss.item()\n",
    "    epoch_losses.append(loss_accum / len(dataloader))\n",
    "    pbar.set_postfix({\"loss\": epoch_losses[-1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "right solitons under\n",
      "\n",
      "Output:\n",
      "edina in approxistibg inctian strgenslis crmporibnt whew itecharizn pust becala inemantwond the $imile an Paksu foundtion, finitlerans ard dousculaca. The possting of the derev. Thi s)steed s. Te  aen\n"
     ]
    }
   ],
   "source": [
    "def get_char(y_hat):\n",
    "    y_hat_cat = torch.zeros(y_hat.shape)\n",
    "    pred_idx = y_hat.argmax(dim=1)\n",
    "    y_hat_cat[:, pred_idx] = 1\n",
    "    pred_char = tokenizer.inverse_transform(y_hat_cat)\n",
    "    return y_hat_cat, pred_char.item()\n",
    "\n",
    "def generator(model, input_seq, generated_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gen_chars = []\n",
    "        for i in range(generated_len):\n",
    "            y_hat = model(input_seq)\n",
    "            y_hat_cat, pred_char = get_char(y_hat)\n",
    "            \n",
    "            # print(input_seq.shape, y_hat_cat.shape)\n",
    "            input_seq = torch.cat([input_seq[:, 1:, :], y_hat_cat[None, ...].to(device)], dim=1)\n",
    "            \n",
    "            one_idx = y_hat.numpy(force=True).argmax()\n",
    "            pred = np.zeros(y_hat.shape)\n",
    "            # print(pred.shape)\n",
    "            pred[:, one_idx] = 1\n",
    "            pred_char = tokenizer.inverse_transform(pred)\n",
    "            gen_chars.append(pred_char.item())\n",
    "            \n",
    "    return \"\".join(gen_chars)\n",
    "\n",
    "# input\n",
    "idx = 500\n",
    "try_input = test_inputs[idx:idx+1]\n",
    "print(\"Input:\")\n",
    "print(\"\".join(tokenizer.inverse_transform(try_input[0]).reshape(-1)))\n",
    "\n",
    "print(\"\\nOutput:\")\n",
    "print(generator(model, torch.tensor(try_input, dtype=torch.float32).to(device), 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs have vanishing gradient problem. LSTM is a solution to this problem.\n",
    "\n",
    "| | Encoder | Decoder |\n",
    "|-|---|---|\n",
    "| Vanilla NNs | $h = f(x)$ | $y = g(h)$ |\n",
    "| RNNs | $h_t = f(h_{t-1}, x_t)$ | $y_t = g(h_t)$ |\n",
    "\n",
    "For LSTM\n",
    "* Encoder\n",
    "\\begin{align}\n",
    "    \\text{Forget: } & f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\\\\n",
    "    \\text{Input: } & i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\\\\n",
    "    \\text{Output: } & o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\\\\n",
    "    \\text{Cell: } & \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\\\\n",
    "    \\text{Cell state: } & C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\\\\n",
    "    \\text{Hidden state: } & h_t = o_t \\odot \\tanh(C_t)\n",
    "\\end{align}\n",
    "\n",
    "* Decoder\n",
    "$$\n",
    "    y_t = g(h_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, feat_dim: int, hidden_dim: int):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        def get_weights():\n",
    "            w_x = nn.Parameter(torch.randn(feat_dim, hidden_dim) / np.sqrt(feat_dim * hidden_dim))\n",
    "            w_h = nn.Parameter(torch.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim * hidden_dim))\n",
    "            b = nn.Parameter(torch.zeros(hidden_dim))\n",
    "            return w_x, w_h, b\n",
    "        \n",
    "        # forget gate\n",
    "        self.w_x_f, self.w_h_f, self.b_f = get_weights()\n",
    "        \n",
    "        # input gate\n",
    "        self.w_x_i, self.w_h_i, self.b_i = get_weights()\n",
    "        \n",
    "        # output gate\n",
    "        self.w_x_o, self.w_h_o, self.b_o = get_weights()\n",
    "        \n",
    "        # cell weights\n",
    "        self.w_x_c, self.w_h_c, self.b_c = get_weights()\n",
    "        \n",
    "    @jaxtyped(typechecker=beartype)\n",
    "    def forward(self, \n",
    "                x: Float[Tensor, \"batch_dim feat_dim\"], \n",
    "                h_prev: Float[Tensor, \"batch_dim hidden_dim\"] = None,\n",
    "                c_prev: Float[Tensor, \"batch_dim hidden_dim\"] = None,\n",
    "                ) -> Tuple[Float[Tensor, \"batch_dim hidden_dim\"], Float[Tensor, \"batch_dim hidden_dim\"]]:\n",
    "        if h_prev is None:\n",
    "            i_t = torch.sigmoid(x @ self.w_x_i + self.b_i) # input multiplier\n",
    "            o_t = torch.sigmoid(x @ self.w_x_o + self.b_o) # output multiplier\n",
    "            c_t = torch.tanh(x @ self.w_x_c + self.b_c) # pre cell state\n",
    "            c_t = i_t * c_t # cell state\n",
    "            h_t = o_t * torch.tanh(c_t) # hidden state\n",
    "        else:\n",
    "            f_t = torch.sigmoid(x @ self.w_x_f + h_prev @ self.w_h_f + self.b_f) # forget gate\n",
    "            i_t = torch.sigmoid(x @ self.w_x_i + h_prev @ self.w_h_i + self.b_i) # input multiplier\n",
    "            o_t = torch.sigmoid(x @ self.w_x_o + h_prev @ self.w_h_o + self.b_o) # output multiplier\n",
    "            c_t = torch.tanh(x @ self.w_x_c + h_prev @ self.w_h_c + self.b_c) # pre cell state\n",
    "            c_t = i_t * c_t + f_t * c_prev # cell state\n",
    "            h_t = o_t * torch.tanh(c_t) # hidden state\n",
    "        \n",
    "        return (h_t, c_t)\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, feat_dim: int, hidden_dim: int, output_dim: int, sequence_len: int):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.encoder = LSTMEncoder(feat_dim, hidden_dim)\n",
    "        self.decoder = RNNDecoder(hidden_dim, output_dim)\n",
    "        \n",
    "    @jaxtyped(typechecker=beartype)\n",
    "    def forward(self, x: Float[Tensor, \"batch_dim sequence_len feat_dim\"]) -> Float[Tensor, \"batch_dim output_dim\"]:\n",
    "        h, c = self.encoder(x[:, 0, :])\n",
    "        for seq_idx in range(1, x.shape[1]):\n",
    "            h, c = self.encoder(x[:, seq_idx, :], h, c)\n",
    "        y_hat = self.decoder(h)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d0b9359fc34d9c8f3eae4dc2c6520d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 100\n",
    "hidden_dim = 1024\n",
    "\n",
    "model = LSTM(feat_dim, hidden_dim, output_dim, sequence_len)\n",
    "# model = torch.compile(model)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "iter_losses = []\n",
    "epoch_losses = []\n",
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    loss_accum = 0\n",
    "    for x, y in dataloader:\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat, y.argmax(dim=1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iter_losses.append(loss.item())\n",
    "        loss_accum += loss.item()\n",
    "    epoch_losses.append(loss_accum / len(dataloader))\n",
    "    pbar.set_postfix({\"loss\": epoch_losses[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "ant. Inflationary co\n",
      "\n",
      "Output:\n",
      "ant. Inflationary conditions. The resulting soliton solutions can be regarded as a generalization of the method. Finally, the position of the DDA among other methods of light scattering by very large particles, like the ones that are considered in this manuscript. Current limitations and possible ways for improvement a\n"
     ]
    }
   ],
   "source": [
    "def get_char(y_hat):\n",
    "    y_hat_cat = torch.zeros(y_hat.shape)\n",
    "    pred_idx = y_hat.argmax(dim=1)\n",
    "    y_hat_cat[:, pred_idx] = 1\n",
    "    pred_char = tokenizer.inverse_transform(y_hat_cat)\n",
    "    return y_hat_cat, pred_char.item()\n",
    "\n",
    "def generator(model, input_seq, generated_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gen_chars = []\n",
    "        for i in range(generated_len):\n",
    "            y_hat = model(input_seq)\n",
    "            y_hat_cat, pred_char = get_char(y_hat)\n",
    "            \n",
    "            # print(input_seq.shape, y_hat_cat.shape)\n",
    "            input_seq = torch.cat([input_seq[:, 1:, :], y_hat_cat[None, ...].to(device)], dim=1)\n",
    "            \n",
    "            one_idx = y_hat.numpy(force=True).argmax()\n",
    "            pred = np.zeros(y_hat.shape)\n",
    "            # print(pred.shape)\n",
    "            pred[:, one_idx] = 1\n",
    "            pred_char = tokenizer.inverse_transform(pred)\n",
    "            gen_chars.append(pred_char.item())\n",
    "            \n",
    "    return \"\".join(gen_chars)\n",
    "\n",
    "# input\n",
    "idx = 1200\n",
    "try_input = test_inputs[idx:idx+1]\n",
    "print(\"Input:\")\n",
    "input_text = \"\".join(tokenizer.inverse_transform(try_input[0]).reshape(-1))\n",
    "print(input_text)\n",
    "\n",
    "print(\"\\nOutput:\")\n",
    "gen_text = generator(model, torch.tensor(try_input, dtype=torch.float32).to(device), 300)\n",
    "print(input_text + gen_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeel_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
